{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_Sentiment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdFQndZfC8XO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1xQ8j3PpN5m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "52435d8c-90dd-4457-f415-6748712b9b41"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)  "
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQQ6FkNcv-ae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = \"/content/gdrive/My Drive/nlp_project/\"\n",
        "train_name = \"spanglish_train_demojised.txt\"\n",
        "validation_name = \"spanglish_validation_demojised.txt\"\n",
        "model_save_name = \"spanglish_model.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThOo8L-9sU7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append(data_path)\n",
        "sys.path.append(data_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hebOrRmGWfj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "007e7da9-189d-4404-f718-e047d56592c3"
      },
      "source": [
        "import torch\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "!pip install transformers"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.47)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.47)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzN9lV1ZGhY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YriUeQu3G47w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "63813253-ae46-4769-da12-a412df79dedb"
      },
      "source": [
        "len(tokenizer.vocab)\n",
        "tokens = tokenizer.tokenize('how ARE you ')\n",
        "\n",
        "print(tokens)\n",
        "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(indexes)\n"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['how', 'are', 'you']\n",
            "[12548, 10320, 10855]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_lDr2XzHEuN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "841c8dc9-89cc-4bde-dcd2-73272f6b86b1"
      },
      "source": [
        "init_token = tokenizer.cls_token\n",
        "eos_token = tokenizer.sep_token\n",
        "pad_token = tokenizer.pad_token\n",
        "unk_token = tokenizer.unk_token\n",
        "\n",
        "print(init_token, eos_token, pad_token, unk_token)\n",
        "\n",
        "\n",
        "# We can get the indexes of the special tokens by converting them using the vocabulary...\n",
        "\n",
        "# In[7]:\n",
        "\n",
        "\n",
        "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
        "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
        "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
        "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
        "\n",
        "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)\n",
        "\n",
        "\n",
        "# ...or by explicitly getting them from the tokenizer.\n",
        "\n",
        "# In[8]:\n",
        "\n",
        "\n",
        "init_token_idx = tokenizer.cls_token_id\n",
        "eos_token_idx = tokenizer.sep_token_id\n",
        "pad_token_idx = tokenizer.pad_token_id\n",
        "unk_token_idx = tokenizer.unk_token_id\n",
        "\n",
        "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)\n",
        "\n",
        "\n",
        "# Another thing we need to handle is that the model was trained on sequences with a defined maximum length - it does not know how to handle sequences longer than it has been trained on. We can get the maximum length of these input sizes by checking the `max_model_input_sizes` for the version of the transformer we want to use. In this case, it is 512 tokens.\n",
        "\n",
        "# In[9]:\n",
        "\n",
        "\n",
        "max_input_length = tokenizer.max_model_input_sizes['bert-base-multilingual-uncased']\n",
        "\n",
        "print(max_input_length)\n",
        "\n",
        "\n",
        "# Previously we have used the `spaCy` tokenizer to tokenize our examples. However we now need to define a function that we will pass to our `TEXT` field that will handle all the tokenization for us. It will also cut down the number of tokens to a maximum length. Note that our maximum length is 2 less than the actual maximum length. This is because we need to append two tokens to each sequence, one to the start and one to the end.\n",
        "\n",
        "# In[10]:\n",
        "\n",
        "\n",
        "def tokenize_and_cut(sentence):\n",
        "    tokens = tokenizer.tokenize(sentence) \n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "# Now we define our fields. The transformer expects the batch dimension to be first, so we set `batch_first = True`. As we already have the vocabulary for our text, provided by the transformer we set `use_vocab = False` to tell torchtext that we'll be handling the vocabulary side of things. We pass our `tokenize_and_cut` function as the tokenizer. The `preprocessing` argument is a function that takes in the example after it has been tokenized, this is where we will convert the tokens to their indexes. Finally, we define the special tokens - making note that we are defining them to be their index value and not their string value, i.e. `100` instead of `[UNK]` This is because the sequences will already be converted into indexes.\n",
        "# \n",
        "# We define the label field as before.\n",
        "\n",
        "# In[87]:\n",
        "\n",
        "\n",
        "from torchtext import data\n"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] [SEP] [PAD] [UNK]\n",
            "101 102 0 100\n",
            "101 102 0 100\n",
            "512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unFFscF5GkpA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2246bf6d-10cf-4915-8c5b-7cfcbc24f950"
      },
      "source": [
        "UID = data.Field(sequential=False, use_vocab=False, pad_token=None)\n",
        "TEXT = data.Field(batch_first = True,\n",
        "                  use_vocab = False,\n",
        "                  tokenize = tokenize_and_cut,\n",
        "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
        "                  init_token = init_token_idx,\n",
        "                  eos_token = eos_token_idx,\n",
        "                  pad_token = pad_token_idx,\n",
        "                  unk_token = unk_token_idx)\n",
        "\n",
        "LABEL = data.LabelField()\n",
        "\n",
        "\n",
        "# We load the data and create the validation splits as before.\n",
        "\n",
        "# In[89]:\n",
        "\n",
        "from torchtext import datasets\n",
        "fields = [('uid',UID),('text', TEXT),('label', LABEL)]\n",
        "train_data, test_data = data.TabularDataset.splits(\n",
        "                                        path = data_path,\n",
        "                                        train = train_name,\n",
        "                                        test = validation_name,\n",
        "                                        format = 'tsv',\n",
        "                                        fields = fields,\n",
        "                                        skip_header = True)\n",
        "\n",
        "# train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "# valid_data = train_data\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(SEED))\n",
        "\n",
        "\n",
        "# In[90]:\n",
        "\n",
        "\n",
        "print(vars(train_data[1]))\n"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'uid': '1915', 'text': [10230, 10251, 11153, 45899, 10140, 11531, 22004, 77003, 10688, 10117, 40886, 25218, 10111, 10102, 47127, 106, 106, 10525, 131, 10181, 16440, 26354, 35523, 10102, 70355, 10536, 29642, 14965, 10952, 15995, 12828, 12828, 10171, 39214, 10108, 27318, 12828, 10171, 39214, 10108, 27318], 'label': '2'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMy50hu2HQKe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "0a0a1d6e-36be-47b6-94a8-6d1503ddf567"
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data)}\")\n",
        "print(f\"Number of testing examples: {len(test_data)}\")\n",
        "print(tokenizer.convert_ids_to_tokens(vars(test_data.examples[331])['text']))\n",
        "\n",
        "\n",
        "# We can check an example and ensure that the text has already been numericalized.\n",
        "\n",
        "# In[92]:\n",
        "\n",
        "\n",
        "print(vars(train_data.examples[6]))\n",
        "\n",
        "\n",
        "# We can use the `convert_ids_to_tokens` to transform these indexes back into readable tokens.\n",
        "\n",
        "# In[93]:\n",
        "\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[6])['text'])\n",
        "\n",
        "print(tokens)\n",
        "\n",
        "\n",
        "# Although we've handled the vocabulary for the text, we still need to build the vocabulary for the labels.\n",
        "\n",
        "# In[94]:\n",
        "\n",
        "\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "\n",
        "# In[95]:\n",
        "\n",
        "\n",
        "print(LABEL.vocab.stoi)\n",
        "\n",
        "\n",
        "# As before, we create the iterators. Ideally we want to use the largest batch size that we can as I've found this gives the best results for transformers.\n",
        "\n",
        "# In[96]:\n",
        "\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 8401\n",
            "Number of validation examples: 3601\n",
            "Number of testing examples: 2998\n",
            "[':', 'highest', 'iq', 'in', 'the', 'world', '.', '.']\n",
            "{'uid': '11003', 'text': [10707, 10624, 10547, 10109, 21105, 10119, 100, 15239, 10453, 100, 12698, 10102, 26201], 'label': '1'}\n",
            "['cu', '##el', '##ga', 'en', 'facebook', 'un', '[UNK]', 'self', '##ie', '[UNK]', 'antes', 'de', 'morir']\n",
            "defaultdict(<function _default_unk_index at 0x7f993d408620>, {'2': 0, '1': 1, '0': 2})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HfcTlJmHWa3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce3f4e3a-f649-4528-fdb6-3eade99fa11e"
      },
      "source": [
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    sort_key=lambda x: len(x.text), \n",
        "    batch_size = BATCH_SIZE, \n",
        "    device = device)\n",
        "\n",
        "size = 0\n",
        "for batch in test_iterator:\n",
        "  #print(batch.uid[0], tokenizer.convert_ids_to_tokens(batch.text[0]))\n",
        "  size += batch.uid.shape[0]\n",
        "print(size)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaX9fJ2BHcUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "bert = BertModel.from_pretrained('bert-base-multilingual-uncased')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1zxSgTlHmkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BERTGRUSentiment(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_dim,\n",
        "                 output_dim,\n",
        "                 n_layers,\n",
        "                 bidirectional,\n",
        "                 dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.bert = bert\n",
        "        \n",
        "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
        "        \n",
        "        self.rnn = nn.GRU(embedding_dim,\n",
        "                          hidden_dim,\n",
        "                          num_layers = n_layers,\n",
        "                          bidirectional = bidirectional,\n",
        "                          batch_first = True,\n",
        "                          dropout = 0 if n_layers < 2 else dropout)\n",
        "        # self.conv = nn.Conv1d(74,74, 3)\n",
        "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        \n",
        "        #text = [batch size, sent len]\n",
        "                \n",
        "        with torch.no_grad():\n",
        "            embedded = self.bert(text)[0]\n",
        "        #print(embedded.size())\n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        \n",
        "        _, hidden = self.rnn(embedded)\n",
        "        #hidden = [n layers * n directions, batch size, emb dim]\n",
        "        #print(hidden.size())\n",
        "        if self.rnn.bidirectional:\n",
        "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "        else:\n",
        "            hidden = self.dropout(hidden[-1,:,:])\n",
        "        #print(hidden.size())\n",
        "        #hidden = [batch size, hid dim]\n",
        "        \n",
        "        output = self.out(hidden)\n",
        "        \n",
        "        #output = [batch size, out dim]\n",
        "        \n",
        "        return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO13qSpBH6Ey",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ffdfa015-2d86-4661-c9be-6775a4500ea9"
      },
      "source": [
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 3\n",
        "N_LAYERS = 3\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0\n",
        "\n",
        "model = BERTGRUSentiment(bert,\n",
        "                         HIDDEN_DIM,\n",
        "                         OUTPUT_DIM,\n",
        "                         N_LAYERS,\n",
        "                         BIDIRECTIONAL,\n",
        "                         DROPOUT)\n",
        "\n",
        "\n",
        "# We can check how many parameters the model has. Our standard models have under 5M, but this one has 112M! Luckily, 110M of these parameters are from the transformer and we will not be training those.\n",
        "\n",
        "# In[100]:\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "\n",
        "# In order to freeze paramers (not train them) we need to set their `requires_grad` attribute to `False`. To do this, we simply loop through all of the `named_parameters` in our model and if they're a part of the `bert` transformer model, we set `requires_grad = False`. \n",
        "\n",
        "# In[101]:\n",
        "\n",
        "\n",
        "for name, param in model.named_parameters():                \n",
        "    if name.startswith('bert'):\n",
        "        param.requires_grad = False\n",
        "\n",
        "\n",
        "# We can now see that our model has under 3M trainable parameters, making it almost comparable to the `FastText` model. However, the text still has to propagate through the transformer which causes training to take considerably longer.\n",
        "\n",
        "# In[102]:\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 171,299,331 trainable parameters\n",
            "The model has 3,942,915 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoF5CJ3oX9jF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1bae0809-17f1-4e22-9e13-be8c17b7cb59"
      },
      "source": [
        "model"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERTGRUSentiment(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (rnn): GRU(768, 256, num_layers=3, batch_first=True, bidirectional=True)\n",
              "  (out): Linear(in_features=512, out_features=3, bias=True)\n",
              "  (dropout): Dropout(p=0, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKjHj_RwH8iD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "e1db2352-5ada-4bc7-8099-3627bfc23fce"
      },
      "source": [
        "\n",
        "for name, param in model.named_parameters():                \n",
        "    if param.requires_grad:\n",
        "        print(name)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rnn.weight_ih_l0\n",
            "rnn.weight_hh_l0\n",
            "rnn.bias_ih_l0\n",
            "rnn.bias_hh_l0\n",
            "rnn.weight_ih_l0_reverse\n",
            "rnn.weight_hh_l0_reverse\n",
            "rnn.bias_ih_l0_reverse\n",
            "rnn.bias_hh_l0_reverse\n",
            "rnn.weight_ih_l1\n",
            "rnn.weight_hh_l1\n",
            "rnn.bias_ih_l1\n",
            "rnn.bias_hh_l1\n",
            "rnn.weight_ih_l1_reverse\n",
            "rnn.weight_hh_l1_reverse\n",
            "rnn.bias_ih_l1_reverse\n",
            "rnn.bias_hh_l1_reverse\n",
            "rnn.weight_ih_l2\n",
            "rnn.weight_hh_l2\n",
            "rnn.bias_ih_l2\n",
            "rnn.bias_hh_l2\n",
            "rnn.weight_ih_l2_reverse\n",
            "rnn.weight_hh_l2_reverse\n",
            "rnn.bias_ih_l2_reverse\n",
            "rnn.bias_hh_l2_reverse\n",
            "out.weight\n",
            "out.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrmNft5kIBWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "# In[105]:\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# Place the model and criterion onto the GPU (if available)\n",
        "\n",
        "# In[106]:\n",
        "\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "\n",
        "# Next, we'll define functions for: calculating accuracy, performing a training epoch, performing an evaluation epoch and calculating how long a training/evaluation epoch takes.\n",
        "\n",
        "# In[107]:\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj5JrQYo31WA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def categorical_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
        "    correct = max_preds.squeeze(1).eq(y)\n",
        "    return correct.sum() / torch.FloatTensor([y.shape[0]])\n",
        "\n",
        "\n",
        "# In[108]:\n",
        "\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        acc = categorical_accuracy(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYbK81uPIKE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            acc = categorical_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "\n",
        "# In[110]:\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt2EwZE_IB57",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4d9196a1-ad1f-4f01-c9a7-b8eb9eddb5dd"
      },
      "source": [
        "N_EPOCHS = 20\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), model_save_name)\n",
        "        path = data_path+F\"{model_save_name}\"\n",
        "        torch.save(model.state_dict(), path)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "\n"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 1.016 | Train Acc: 47.60%\n",
            "\t Val. Loss: 0.976 |  Val. Acc: 51.07%\n",
            "Epoch: 02 | Epoch Time: 0m 36s\n",
            "\tTrain Loss: 0.972 | Train Acc: 50.91%\n",
            "\t Val. Loss: 0.963 |  Val. Acc: 51.34%\n",
            "Epoch: 03 | Epoch Time: 0m 36s\n",
            "\tTrain Loss: 0.938 | Train Acc: 53.09%\n",
            "\t Val. Loss: 0.932 |  Val. Acc: 53.40%\n",
            "Epoch: 04 | Epoch Time: 0m 37s\n",
            "\tTrain Loss: 0.920 | Train Acc: 54.38%\n",
            "\t Val. Loss: 0.929 |  Val. Acc: 54.00%\n",
            "Epoch: 05 | Epoch Time: 0m 37s\n",
            "\tTrain Loss: 0.898 | Train Acc: 56.51%\n",
            "\t Val. Loss: 0.931 |  Val. Acc: 53.94%\n",
            "Epoch: 06 | Epoch Time: 0m 37s\n",
            "\tTrain Loss: 0.870 | Train Acc: 57.99%\n",
            "\t Val. Loss: 0.945 |  Val. Acc: 53.51%\n",
            "Epoch: 07 | Epoch Time: 0m 37s\n",
            "\tTrain Loss: 0.838 | Train Acc: 61.50%\n",
            "\t Val. Loss: 1.002 |  Val. Acc: 53.17%\n",
            "Epoch: 08 | Epoch Time: 0m 37s\n",
            "\tTrain Loss: 0.804 | Train Acc: 62.64%\n",
            "\t Val. Loss: 1.004 |  Val. Acc: 53.88%\n",
            "Epoch: 09 | Epoch Time: 0m 36s\n",
            "\tTrain Loss: 0.747 | Train Acc: 66.15%\n",
            "\t Val. Loss: 1.070 |  Val. Acc: 53.59%\n",
            "Epoch: 10 | Epoch Time: 0m 37s\n",
            "\tTrain Loss: 0.685 | Train Acc: 69.38%\n",
            "\t Val. Loss: 1.125 |  Val. Acc: 49.55%\n",
            "Epoch: 11 | Epoch Time: 0m 37s\n",
            "\tTrain Loss: 0.631 | Train Acc: 72.39%\n",
            "\t Val. Loss: 1.227 |  Val. Acc: 51.71%\n",
            "Epoch: 12 | Epoch Time: 0m 37s\n",
            "\tTrain Loss: 0.558 | Train Acc: 76.24%\n",
            "\t Val. Loss: 1.232 |  Val. Acc: 51.12%\n",
            "Epoch: 13 | Epoch Time: 0m 37s\n",
            "\tTrain Loss: 0.491 | Train Acc: 79.04%\n",
            "\t Val. Loss: 1.483 |  Val. Acc: 52.81%\n",
            "Epoch: 14 | Epoch Time: 0m 37s\n",
            "\tTrain Loss: 0.434 | Train Acc: 81.91%\n",
            "\t Val. Loss: 1.553 |  Val. Acc: 51.63%\n",
            "Epoch: 15 | Epoch Time: 0m 37s\n",
            "\tTrain Loss: 0.394 | Train Acc: 83.33%\n",
            "\t Val. Loss: 1.721 |  Val. Acc: 49.39%\n",
            "Epoch: 16 | Epoch Time: 0m 37s\n",
            "\tTrain Loss: 0.332 | Train Acc: 86.27%\n",
            "\t Val. Loss: 1.798 |  Val. Acc: 49.81%\n",
            "Epoch: 17 | Epoch Time: 0m 36s\n",
            "\tTrain Loss: 0.317 | Train Acc: 86.76%\n",
            "\t Val. Loss: 1.926 |  Val. Acc: 49.36%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-8d58fe863ecb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-99-aca4bb17649c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategorical_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-99-aca4bb17649c>\u001b[0m in \u001b[0;36mcategorical_accuracy\u001b[0;34m(preds, y)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmax_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# get the index of the max probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWLxw5OIp2HB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9b25970d-f797-4a22-cdbd-893860bb654c"
      },
      "source": [
        "path = data_path+F\"{model_save_name}\"\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEmZVcLs4Cws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def test_evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    epoch_all_acc = torch.FloatTensor([0,0,0,0,0,0])\n",
        "    print(epoch_all_acc)\n",
        "    model.eval()\n",
        "\n",
        "    label_dict = {0:\"positive\", 1:\"neutral\", 2:\"negative\"}\n",
        "    answer = open(\"answer.txt\", \"w\")\n",
        "    answer.write(\"Uid,Sentiment\\n\")\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            #write to file\n",
        "            #print(\"##############\")\n",
        "            #print(batch.uid.shape)\n",
        "            #print(predictions.shape)\n",
        "            for i in range(batch.uid.shape[0]):\n",
        "              uid = batch.uid[i].item()\n",
        "              label_number = torch.argmax(predictions[i]).item()\n",
        "              label_string = label_dict[label_number]\n",
        "              #print(uid, label_string)\n",
        "              answer.write(str(uid)+','+label_string+'\\n')\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            acc,all_acc = test_categorical_accuracy(predictions, batch.label)\n",
        "            print(all_acc)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "            epoch_all_acc += all_acc\n",
        "        \n",
        "    answer.close()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator),epoch_all_acc/len(iterator)\n",
        "\n",
        "def test_categorical_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    count0,count1,count2 = torch.zeros(1),torch.zeros(1),torch.zeros(1)\n",
        "    total0,total1,total2 = torch.FloatTensor(1),torch.FloatTensor(1),torch.FloatTensor(1)\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
        "    correct = max_preds.squeeze(1).eq(y)\n",
        "    \n",
        "    for j,i in enumerate(y.cpu().numpy()):\n",
        "      if i==0:\n",
        "        count0+=correct[j]\n",
        "        total0+=1\n",
        "      elif i==1:\n",
        "        count1+=correct[j]\n",
        "        total1+=1\n",
        "      elif i==2:\n",
        "        count2+=correct[j]\n",
        "      else:\n",
        "        print(i,i==0,i==1,i==2)\n",
        "        total2+=1\n",
        "    # print(count0,count1,count2,total0,total1,total2)\n",
        "    # print([count0/total0,count1/total1,count2/total2])\n",
        "    # print(torch.FloatTensor([count0/total0,count1/total1,count2/total2]))\n",
        "    # print(correct.sum() / torch.FloatTensor([y.shape[0]]))\n",
        "    # print(torch.FloatTensor([count0/total0,count1/total1,count2/total2]))\n",
        "    print(count0,count1,count2)\n",
        "    return correct.sum() / torch.FloatTensor([y.shape[0]]),torch.FloatTensor([count0/total0,count1/total1,count2/total2,count0,count1,count2])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IixrBAcbJeLJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "228f6898-efc9-44ef-cc0d-37809948237f"
      },
      "source": [
        "# model.load_state_dict(torch.load('tut6-model.pt'))\n",
        "\n",
        "test_loss, test_acc,test_all_acc = test_evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%',test_all_acc)\n",
        "\n",
        "\n",
        "# ## Inference\n",
        "# \n",
        "# We'll then use the model to test the sentiment of some sequences. We tokenize the input sequence, trim it down to the maximum length, add the special tokens to either side, convert it to a tensor, add a fake batch dimension and then pass it through our model.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0., 0., 0., 0., 0., 0.])\n",
            "tensor([40.]) tensor([19.]) tensor([0.])\n",
            "tensor([ 0.8163,  0.2603, -0.0000, 40.0000, 19.0000,  0.0000])\n",
            "tensor([52.]) tensor([10.]) tensor([5.])\n",
            "tensor([ 8.6667e-01,  1.9608e-01, -1.4826e+15,  5.2000e+01,  1.0000e+01,\n",
            "         5.0000e+00])\n",
            "tensor([36.]) tensor([9.]) tensor([2.])\n",
            "tensor([ 7.2000e-01,  1.5517e-01, -5.7793e+14,  3.6000e+01,  9.0000e+00,\n",
            "         2.0000e+00])\n",
            "tensor([53.]) tensor([6.]) tensor([6.])\n",
            "tensor([ 8.8333e-01,  1.3043e-01, -1.9778e+15,  5.3000e+01,  6.0000e+00,\n",
            "         6.0000e+00])\n",
            "tensor([40.]) tensor([5.]) tensor([7.])\n",
            "tensor([ 7.0175e-01,  9.8039e-02, -2.0751e+15,  4.0000e+01,  5.0000e+00,\n",
            "         7.0000e+00])\n",
            "tensor([54.]) tensor([10.]) tensor([5.])\n",
            "tensor([ 9.1525e-01,  2.0000e-01, -1.4820e+15,  5.4000e+01,  1.0000e+01,\n",
            "         5.0000e+00])\n",
            "tensor([48.]) tensor([8.]) tensor([5.])\n",
            "tensor([ 9.0566e-01,  1.4545e-01, -1.6480e+15,  4.8000e+01,  8.0000e+00,\n",
            "         5.0000e+00])\n",
            "tensor([42.]) tensor([8.]) tensor([4.])\n",
            "tensor([ 7.6364e-01,  1.5385e-01, -1.1748e+15,  4.2000e+01,  8.0000e+00,\n",
            "         4.0000e+00])\n",
            "tensor([47.]) tensor([9.]) tensor([4.])\n",
            "tensor([ 7.7049e-01,  2.0455e-01, -1.1753e+15,  4.7000e+01,  9.0000e+00,\n",
            "         4.0000e+00])\n",
            "tensor([54.]) tensor([7.]) tensor([10.])\n",
            "tensor([ 8.4375e-01,  1.7949e-01, -3.2969e+15,  5.4000e+01,  7.0000e+00,\n",
            "         1.0000e+01])\n",
            "tensor([46.]) tensor([12.]) tensor([9.])\n",
            "tensor([ 7.0769e-01,  2.8571e-01, -2.6687e+15,  4.6000e+01,  1.2000e+01,\n",
            "         9.0000e+00])\n",
            "tensor([56.]) tensor([8.]) tensor([3.])\n",
            "tensor([ 8.1159e-01,  2.0000e-01, -8.7498e+14,  5.6000e+01,  8.0000e+00,\n",
            "         3.0000e+00])\n",
            "tensor([51.]) tensor([4.]) tensor([13.])\n",
            "tensor([ 8.0952e-01,  1.0000e-01, -3.8545e+15,  5.1000e+01,  4.0000e+00,\n",
            "         1.3000e+01])\n",
            "tensor([58.]) tensor([0.]) tensor([9.])\n",
            "tensor([ 8.2857e-01,  0.0000e+00, -2.6007e+15,  5.8000e+01,  0.0000e+00,\n",
            "         9.0000e+00])\n",
            "tensor([43.]) tensor([6.]) tensor([13.])\n",
            "tensor([ 7.1667e-01,  1.5000e-01, -4.2853e+15,  4.3000e+01,  6.0000e+00,\n",
            "         1.3000e+01])\n",
            "tensor([45.]) tensor([7.]) tensor([7.])\n",
            "tensor([ 6.7164e-01,  1.7949e-01, -2.3072e+15,  4.5000e+01,  7.0000e+00,\n",
            "         7.0000e+00])\n",
            "tensor([52.]) tensor([11.]) tensor([7.])\n",
            "tensor([ 7.6471e-01,  2.6190e-01, -2.0751e+15,  5.2000e+01,  1.1000e+01,\n",
            "         7.0000e+00])\n",
            "tensor([47.]) tensor([8.]) tensor([9.])\n",
            "tensor([ 6.8116e-01,  2.1053e-01, -2.6007e+15,  4.7000e+01,  8.0000e+00,\n",
            "         9.0000e+00])\n",
            "tensor([54.]) tensor([9.]) tensor([5.])\n",
            "tensor([ 7.3973e-01,  2.5714e-01, -1.4825e+15,  5.4000e+01,  9.0000e+00,\n",
            "         5.0000e+00])\n",
            "tensor([56.]) tensor([12.]) tensor([11.])\n",
            "tensor([ 7.3684e-01,  3.6364e-01, -3.2618e+15,  5.6000e+01,  1.2000e+01,\n",
            "         1.1000e+01])\n",
            "tensor([54.]) tensor([9.]) tensor([11.])\n",
            "tensor([ 7.1053e-01,  3.0000e-01, -3.2083e+15,  5.4000e+01,  9.0000e+00,\n",
            "         1.1000e+01])\n",
            "tensor([50.]) tensor([8.]) tensor([7.])\n",
            "tensor([ 7.3529e-01,  2.4242e-01, -2.3079e+15,  5.0000e+01,  8.0000e+00,\n",
            "         7.0000e+00])\n",
            "tensor([52.]) tensor([5.]) tensor([13.])\n",
            "tensor([ 7.0270e-01,  2.2727e-01, -3.8196e+15,  5.2000e+01,  5.0000e+00,\n",
            "         1.3000e+01])\n",
            "tensor([29.]) tensor([2.]) tensor([7.])\n",
            "tensor([ 9.0625e-01,  1.6667e-01, -2.3075e+15,  2.9000e+01,  2.0000e+00,\n",
            "         7.0000e+00])\n",
            "Test Loss: 0.962 | Test Acc: 51.27% tensor([ 7.7957e-01,  1.9450e-01, -2.1894e+15,  4.8292e+01,  8.0000e+00,\n",
            "         7.1667e+00])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONmKOMZmJtfk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_sentiment(model, tokenizer, sentence):\n",
        "    model.eval()\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(0)\n",
        "    prediction = torch.sigmoid(model(tensor))\n",
        "    # ind = np.argmax(np.array(prediction))\n",
        "    # if ind ==0:\n",
        "    #   print('neutral')\n",
        "    # elif ind == 1:\n",
        "    #   print(\"positive\")\n",
        "    # else:\n",
        "    #   print(\"negative\")\n",
        "    print(prediction)\n",
        "predict_sentiment(model, tokenizer, \"This film is terrible\")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "predict_sentiment(model, tokenizer, \"This film is great\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SO-V3S1HV2Oi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "while True:\n",
        "  sent = input('->')\n",
        "  if sent != '$':\n",
        "    predict_sentiment(model, tokenizer, sent)\n",
        "  else:\n",
        "    break\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}